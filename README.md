# k-NN Classification on Iris Dataset 🌸

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/akshatdanve/iris-kNN-classification/blob/main/k_NN_Classification_on_Iris_Dataset.ipynb)

This project applies the **k-Nearest Neighbors (k-NN)** algorithm to classify the famous **Iris dataset** into its three species: *Setosa*, *Versicolor*, and *Virginica*. It walks through the end-to-end process of data preprocessing, model training, evaluation, and visualization — all implemented in **Google Colab**.

---

## 📌 Project Overview

* Implemented the **k-NN classifier** using `scikit-learn`.
* Explored **data visualization** to understand feature distributions.
* Tuned the value of **k** to optimize model accuracy.
* Evaluated the model using **confusion matrix**, **classification report**, and accuracy score.
* Entire workflow is neatly documented and reproducible in a Colab notebook.

This project demonstrates how a simple, distance-based algorithm like k-NN can achieve strong performance on a classical dataset.

---

## 📂 Repository Structure

```
├── k_NN_Classification_on_Iris_Dataset.ipynb   # Google Colab Notebook with full implementation
├── README.md                                   # Project documentation
```

---

## 📊 Dataset

* The [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) is one of the most well-known datasets in machine learning.
* Features:

  * Sepal length
  * Sepal width
  * Petal length
  * Petal width
* Target:

  * Iris-setosa
  * Iris-versicolor
  * Iris-virginica

---

## 🚀 Usage

1. Open the notebook directly in **Google Colab**:

   * Upload the `.ipynb` file to your Google Drive
   * Right-click → *Open with* → *Google Colaboratory*

2. Run all cells to reproduce results.
   You’ll see:

   * Data exploration and visualization
   * Model training with k-NN
   * Performance evaluation

---

## 📈 Results

* Achieved **~95–97%** accuracy with the optimal **k** value.
* Demonstrated strong classification performance and clear decision boundaries between species.
* Shows how k-NN handles multi-class classification effectively.

---

## 🔑 Key Learnings

* The role of **feature scaling** in distance-based algorithms.
* How the choice of **k** affects bias-variance trade-off.
* Visual insights into class separability in the Iris dataset.

---

## 🛠️ Technologies Used

* Google Colab (Python 3)
* NumPy
* Pandas
* Matplotlib / Seaborn
* scikit-learn

---

## 🤝 Contributing

Contributions, issues, and feature requests are welcome!
Feel free to fork this repo and submit a pull request.

---

## 🙏 Acknowledgements

* The [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) from the UCI Machine Learning Repository
* [scikit-learn](https://scikit-learn.org/stable/) for machine learning utilities
* [Matplotlib](https://matplotlib.org/) and [Seaborn](https://seaborn.pydata.org/) for data visualization
* [Google Colab](https://colab.research.google.com/) for providing a free and convenient development environment

---

## 👨‍💻 Author

**Akshat Danve**

* 🌐 [GitHub](https://github.com/akshatdanve)
* 🔗 [LinkedIn](https://www.linkedin.com/in/akshat-danve/)
* ✉️ [Email](mailto:akshatdanve47@gmail.com)

---


